# Heart-Disease-Prediction
A Project on Heart disease Prediction using Data Mining Techniques






Dissertation Title
Predicting Heart Disease Utilizing Data Mining Techniques


Final Thesis

In Partial Fulfillment
of the Requirements for the Degree of
Master in Computer Science




Student Name	:	Steve Ranjeet Jain
Student ID	:	u2349486
Supervisor	:	Dr Seyed Ali Ghorashi











 
Abstract
Even though medical societies produce enormous amounts of information (healing centers, therapeutic focuses), this knowledge is rarely appropriately utilized. The health care industry is "data-rich" yet "knowledge-poor." There is no reliable way for assessing relationships and patterns in health care data. Diagnoses made by physicists that are based on human expertise are subject to error. Through this investigation, diverse data mining approaches are examined to establish an advanced prognostic model for heart disease, harnessing the potential of comprehensive clinical datasets. The study centers on harnessing data mining methodologies to create a sophisticated medical system for heart disease diagnosis, integrating patient-specific information and key risk factors. Medical professionals can help people anticipate cardiac disease. These techniques are used to analyze vast amounts of data generated by medical diagnosis in order to extract meaningful knowledge, often known as expertise. In order to better comprehend medical data and avoid heart disease, mining is a technique for examining huge data sets to locate hidden and previously undiscovered relationships and information patterns. Classification methods such as Naive Bayes (NB), Decision Trees (DT), Neural Networks (NN), Artificial Intelligence (AI), and Clustering Algorithms such as K-Nearest Neighbors (KNN) can be used. It is advised to use a variety of data mining approaches, such as Deep Machine Learning, Logistic Regression, Decision Trees, and the Random Forest Classifier. To decide which technique is more suitable for enhancing machine performance, use a neural network classifier. A decision support system is an interactive computer-based structure that uses data and models to assist decision-makers in identifying difficulties and making decisions. The Heart Disease Prediction System's objective is to analyze and comprehend the available data mining prediction models quickly and easily. Comparison reveals the degree of accuracy of each model.


Keywords: 	Data mining; Decision tree; Heart-disease; K Nearest Neighbor; Logistic regression; Random Forest; 
 
Acknowledgements
I would like to take this chance to express my gratitude to my professor for his assistance during the assignment. I would like to offer my heartfelt gratitude to everyone who assisted me in doing a fantastic job. I am quite grateful to "Dr. Seyed Ali Ghorashi" for his guidance, encouragement, continuous monitoring, and necessary project information.
Finally, I want to express my gratitude to everyone who helped with this project, whether directly or indirectly. Your encouragement, suggestions, and support have helped me to develop my thoughts and raise the standard of this dissertation. I would want to close by expressing my sincere gratitude to everyone who helped me with this study project. I genuinely appreciate you being a part of my academic path because your contributions have been important. 
Contents
Abstract	ii
Acknowledgements	iii
List of Tables	v
List of Figures	vi
List of Acronyms	ix
Chapter 1	Introduction	1
1.1	Background	2
1.2	Problem Statement	3
1.3	Research Question and Objectives	4
1.4	Expected outcomes	6
Chapter 2	7
Literature Review/Related Work.	7
2.1	Comprehensive Overview of the Existing Literature	7
2.2	Critical Analysis of Existing Studies	9
Chapter 3	Methodology	10
3.1	Data Collection and Preprocessing [18]	12
3.2	ML/AI Model Development	19
3.2.1	Logistic Regression [15]	19
3.2.2	Decision tree [16,17]	21
3.2.3	Random Forest Classifier [18]	24
3.2.4	K Nearest Neighbor:[19]	26
3.3	Evaluation of the Proposed System	30
Chapter 4	32
Experimental Results	32
4.1	Experimental Setup	32
4.1.1	Hardware Setup	32
4.1.2	Software Setup	33
4.1.3	Exploratory Data Analysis [23]	33
4.2	Results	75
4.2.1	Training and Testing	75
4.2.2	Logistic Regression	76
4.2.3	Decision tree	79
4.2.4	K Nearest Neighbor	81
4.2.5	Random forest	84
4.3	Comparison Between the Data Mining Techniques	87
Chapter 5	Conclusion and Future Work	90
5.1	Conclusion	90
5.2	Future Work	90
References	92
Appendix A.	 Title of Appendix	95
Appendix B.	 Title of Appendix	96

List of Tables
Table 1 Critical analysis/ Summary of the existing studies	9
Table 2 DataSet Description	14
Table 3 Discrete vs Continuous Variable	49
Table 4 Categorical Variables	50
Table 5 Comparison	87


 
List of Figures
Figure 1 Methodology	11
Figure 2 Example of decision tree	22
Figure 3 Decision Tree	23
Figure 4 Working of Random Forest	25
Figure 5 Example of KNN	28
Figure 6 Loading Libraries	34
Figure 7 First Data Set	35
Figure 8 Second Data Set	35
Figure 9 Changed condition to target	36
Figure 10 Third Data Set	36
Figure 11 Fourth Data Set	37
Figure 12 Fifth Data Set	37
Figure 13 Sixth Data Set	38
Figure 14 Mapped sex and target columns	38
Figure 15 Summary of the First Dataset	39
Figure 16 Summary of the Second Data Set	39
Figure 17 Summary of the Third Data Set	40
Figure 18 Summary of the Fourth Data Set	40
Figure 19 Summary of the Fifth Data Set	40
Figure 20 Summary of the Sixth Data Set	41
Figure 21 Merged Data Set	41
Figure 22 Finding out null values and dropping null values in my dataset	42
Figure 23 Summary of the Merged Data Set	43
Figure 24 Used info() to get information of the table	43
Figure 25 Shape and Columns function	44
Figure 26 Printing unique values of each column(1)	45
Figure 27 Printing unique values of each column(2)	46
Figure 28 Printing unique values of each column(3)	47
Figure 29 Printing unique values of each column(4)	48
Figure 30 Process of the code for correlation matrix	51
Figure 31 Output of the above code	51
Figure 32 Correlation Coefficient	52
Figure 33 Histogram (1)	53
Figure 34 Histogram (2)	53
Figure 35 The balanced between the Data set	54
Figure 36 percentage of the target attribute	54
Figure 37 Working on the ‘age’ attribute	56
Figure 38 Working on the ‘sex’ attribute	57
Figure 39 ‘sex’ vs ‘target’	58
Figure 40 A graphical representation of “Heart Disease Frequency for Sex”	59
Figure 41 “Plotting the Fbs Attribute”	60
Figure 42 “Plotting the exang Attribute”	61
Figure 43 Plotting the cp(1)	62
Figure 44 Plotting the cp(2)	63
Figure 45 Working with “restecg”	64
Figure 46 Working with “slope”	65
Figure 47 Working with “ca”	66
Figure 48 Working with thal”	67
Figure 49 Working with “Oldpeak”	68
Figure 50 working with “Cholesterol”	69
Figure 51 Working with “Thalach”	70
Figure 52 Choosing the best 5 features	72
Figure 53 Train and Test	73
Figure 54 Using Standard Scalar	74
Figure 55 Training and testing the Data Set	75
Figure 56 Logistic Regression	76
Figure 57 Confusion matrix	76
Figure 58 F1 score	77
Figure 59 Specificity	77
Figure 60 Plotting the confusion matrix	78
Figure 61 Decision tree	79
Figure 62 Using GridSearchCv to tune my decision tree(1)	79
Figure 63 Using GridSearchCv to tune my decision tree(2)	79
Figure 64 Decision Tree with (parameters)	80
Figure 65 Code for KNN	81
Figure 66 Plotting the KNN algorithm(1)	82
Figure 67 Plotting the KNN algorithm (2)	83
Figure 68 Printing the scores for KNN	84
Figure 69 The mean score	84
Figure 70 Random Forest Generator	84
Figure 71 Mean Score For RF	85
Figure 72 Using GridSearchCv for RF(1)	85
Figure 73 Using GridSearchCv for RF(2)	85
Figure 74 RF with Parameters	86
Figure 75 Process of the code	86
Figure 76 Plotting Random Forest Using No. Of Columns	87
Figure 77 Classifier	89








 









List of Acronyms
Term	Full Forms  
CVD 	Cardiovascular Disease 
BD	Big Data
ECG	electrocardiogram
ECHO	echocardiogram
DT	Decision tree	
RFC	Random Forest Classifier
LR	Logistic Regression	
KNN	K Nearest Neighbor




 
Chapter 1	
Introduction
Cardiac disease is said to be the world's leading cause of death and morbidity, placing a heavy strain on people, families, and healthcare systems. Predicting a person's risk of developing heart disease is essential for early identification, prevention, and targeted therapies. Data mining techniques have evolved over time, giving academics strong tools for analyzing huge, complicated information and facilitating the creation of precise prediction models. Advances in technology and the increased availability of health-related data have opened new opportunities for enhancing the accuracy and efficiency of cardiac disease prediction in recent years. By leveraging the potential of data mining tools, this dissertation tries to bridge the gap between old risk assessment methodologies and current data-driven alternatives. This study aims to find hidden patterns and associations in big and complex datasets including multiple health metrics, genetic variables, lifestyle decisions, and medical histories in order to improve our understanding of heart disease development. As our understanding of cardiac illness gets more detailed, the need of personalized treatment has grown. The use of data mining not only helps us to detect high-risk patients on a larger scale, but it also enables the development of personalized therapies that respond to each patient's particular needs. This study aims to construct complete prediction models that not only excel in accuracy but also give insights into the underlying mechanisms that contribute to heart disease by merging multiple algorithms and approaches.
This dissertation's main goal is to investigate how data mining techniques may be used to anticipate cardiac disease. Our goal is to create strong and trustworthy models that are capable of precisely identifying people who are at high risk for heart disease by utilizing a variety of algorithms and methodologies. These prediction models may help with therapeutic decision-making, personalize interventions, and ultimately optimize patient outcomes. Furthermore, the findings of this study have the potential to transform clinical practice by providing physicians with timely and data-supported insights. These insights can help healthcare providers make sound judgements about preventative measures, early intervention, and treatment techniques. By tackling the issues related with data quality, feature selection, and model interpretability, this study aims to enhance the science of cardiac illness prediction and pave the way for a future in which proactive and personalized cardiac treatment is the norm. Finally, the use of strong prediction models powered by data mining can help to improve patient outcomes, reduce the load on healthcare systems, and improve the overall quality of life for those at risk of or impacted by heart disease.
 




1.1	Background 
There are enormous amounts of unprocessed medical data in the health sector. Medical data from patients contain hidden patterns that are crucial for the research of cardiac illness. In the past 15 years, heart disease has been the leading cause of death worldwide. An important organ in the human body is the heart. If the body's blood flow is insufficient, the brain and heart stop working, and death occurs within a few minutes. Different scenarios might enhance one's sensitivity to various health concerns. Age, genetic predisposition, diabetes, increased cholesterol levels, hypertension, excessive alcohol consumption, smoking, obesity, sedentary lifestyle, a variety of chest-related discomforts, and an insufficient nutrition are among these risk factors. It's critical to understand that living a healthy lifestyle and addressing these risk factors can play a critical role in reducing these possible dangers. It is possible to separate information obtained from the patient's medical file and provide a report on heart illness that indicates whether it is positive or negative. The diagnosis of heart disease is influenced by both factors. The results of the patient's electrocardiogram (ECG), echocardiogram (ECHO) examination, and medical expertise are generally used to make the diagnosis. The heart's electrical activity is captured by electrodes mounted on the patient's body. This signal depolarizes with each pulse that is reflected by the electrical activity on the skin. The ECHO depolarization test, which uses ultrasonic waves to replicate the heart's muscles and is used to identify cardiovascular disease (CVD), is held by the heart muscles. It is one of the harder diagnoses and requires extensive background information. Data mining is the tacit extraction of previously undiscovered potentially helpful information that uses sophisticated algorithms to become aware of medical data. Large-scale data collecting and archiving is what big data (BD) is. Big Data and Data Mining are two related concepts. Both strategies play a comparable function in the development of data reports by gathering and analyzing pertinent data. Data mining is fundamentally an activity to find patterns in connected knowledge and data using big data. With the help of this extensive data analysis procedure, valuable patterns and undiscovered relationships with hidden patterns are analytically studied to help decision-makers.
1.2	Problem Statement
Most hospital’s information systems are made to make it easier to manage inventory, care for patients, and generate simple statistics. While largely limited, some hospitals deploy decision help programs. You should provide basic information in response to inquiries like "What is the average age of these patients?" "How many surgeries resulted in hospital stays longer than 15 days?" "Identify female patients who are single, above the age of 30, and have undergone treatment for either cancer or heart disease. However, it is important to note that the system may not be equipped to address complex queries such as 'Determine the significant predictors prior to surgery that contribute to longer hospital stays.' Additionally, tasks like 'Decide whether cancer patients should receive chemotherapy alone, radiation alone, or a combination of both, based on their records,' and 'Predict the likelihood of patients developing heart disease using records of cancer patients,' might exceed the current capabilities of the system. It's recommended to consider the system's limitations while formulating your research and questions."
Health assessments are always based on doctors' instincts and knowledge, not the huge information available in the database. When charged with tasks like finding important determinants of treatment results or proposing ideal tactics for individuals with numerous illnesses, the system's shortcomings become very apparent. This disparity is even more problematic when one considers that healthcare choices are mostly based on the clinical intuition of medical practitioners, which frequently disregard the amount of data housed inside these systems. This information gap between data availability and clinical decision-making creates bias, mistakes, and increased medical costs. As a result, there is an urgent need to bridge this gap and harness the power of computer-based patient information to improve decision support in medical settings. This procedure results in unwarranted bias, errors, and expensive medical expenses, which have an impact on patients' level of care. Using computer-based patient records to aid clinical decision-making, may help to reduce medical errors, improve patient safety, reduce pointless practice differences, and improve patient outcomes. This endeavor is intriguing because it will develop a knowledge-rich environment that can significantly improve the efficiency of clinical judgements through data processing and research approaches like data mining. The biggest difficulty connected with heart disease is detecting it. It is not always simple to tell if someone has heart disease. While there are techniques available to estimate the probability of heart disease, they have certain limitations. Some of these instruments are rather expensive, keeping them out of reach for many people. On the other hand, certain methods are ineffective at properly predicting the risk of heart disease in people. This is a considerable obstacle to early detection and preventative interventions. The mortality rate and total consequences can be reduced by early identification of heart disorders [1].



1.3	Research Question and Objectives

1.3.1 Research Question
How can data be transformed into knowledge that helps medical professionals make wise clinical decisions?

1.3.2 Motivation 
The development of technology has altered the planet. One of the most expensive losses to consider is wasted time and money. The provision of high-quality care at an affordable cost is a big challenge for healthcare organizations (hospitals, emergency rooms). Professional care comprises a thorough patient evaluation and the right treatments. Poor professional judgements will produce catastrophic, irrational results. Hospitals must thus minimize the hazards associated with pharmacological studies. The employment of appropriate computer-based information systems and/or decision support systems can lead to these results. Most hospitals in use today monitor their patient or health data using hospital management systems. These systems generate enormous amounts of data in the form of graphs, text, diagrams, and images. Sadly, these findings are frequently utilized to guide therapeutic judgement. These particulars include a wealth of sensitive, often untapped information.


1.3.3 Objectives
I.	Create a comprehensive dataset for heart disease prediction by combining many data sources, such as medical records, lifestyle variables, genetic information, and clinical features. To get the dataset ready for analysis, preprocess it by cleaning it, combining it with data from other sources, choosing the right features, doing feature engineering, and normalizing or encoding the data. 
II.	Model construction using data mining techniques: Create a prediction model that takes a range of risk factors into account using data mining techniques including statistics and machine learning algorithms. Determine patterns, correlations, and interactions between various risk factors by analyzing the collected data. In order to properly forecast the likelihood of acquiring heart disease, select the finest data mining techniques after thorough study and review.
III.	Evaluation and dissemination: Compare the developed predictive model to established techniques for predicting cardiac disease. Utilize criteria like accuracy, precision, and recall to evaluate the model's performance. Examine the model's applicability in addition by outlining the predictions made in order to increase trust and openness in the decision-making process. 
IV.	Publishing research findings in respected scientific publications and disseminating knowledge about new developments in heart disease prediction.




1.4	Expected outcomes
This dissertation's practical component will be described in depth, covering the dataset's specifications, preprocessing techniques, model architectures, hyperparameters, evaluation metrics, algorithms’ implementations in Python, and results. Tables, graphs, and figures can all be utilized to present the findings clearly. This evidence can be included in a research report, thesis, or academic publication and will be an invaluable resource for future use. Classification models are used in this project to categorize private, unordered values or results, whilst prediction models try to foretell future values. Prediction algorithms include rules of association and clustering, which focus on recognizing patterns and relationships in data. Classification and regression models, such as decision trees and neural networks, are used for tasks involving categorization and quantitative estimation. These strategies, when combined, create a set of tools that enable intelligent data analysis and decision-making across a wide range of applications. Using these methods, insights can be retrieved from complicated datasets, assisting in both comprehending underlying structures and creating informed forecasts for diverse circumstances. The following data mining categorization methods will be utilized in this forecast for cardiac conditions:
•	Logistic Regression
•	Decision Tree
•	Random Forest Classifier
•	K-Nearest Neighbor




 
Chapter 2	
Literature Review/Related Work. 

2.1	Comprehensive Overview of the Existing Literature 
Data mining techniques have attracted a lot of attention recently because they have the potential to enhance early detection and prevention methods for heart disease. This review of the literature gives an overview of the available studies, identifies their shortcomings, and suggests further research to solve those deficiencies. 
Heart disease prediction has been explored using a variety of data mining techniques. A thorough analysis of data mining methods used to forecast cardiac disease was undertaken by Kumar and Tripathy [2]. Popular methods like decision trees, support vector machines, artificial neural networks were covered. The evaluation focused on the benefits and drawbacks of each technology and offered details on how it might be used to forecast heart disease. 
Tim Turner and Mai Shouman [3] talked about the shortcomings in research on the analysis and detection of cardiovascular diseases and proposed a tool that methodically addresses those shortcomings in order to test whether data mining techniques can produce results that are as precise as those found in the detection of cardiovascular diseases. The diagnostic accuracies of single and hybrid data mining techniques for CHDD cardiomyopathy are different, with hybrid strategies being more accurate than single procedures. This research proposes a model for evaluating whether the management of heart disease data using data mining approaches may produce reliable results in the diagnosis of individuals with heart disease.
Convolutional neural networks (CNNs) and recurrent neural networks (RNNs) have showed promise in the prediction of cardiac disease. A deep learning model was created by Rajkomar et al. [4] to forecast cardiovascular risk variables from data from electronic health records. The model obtained great accuracy in predicting the likelihood of developing heart disease by combining CNNs and RNNs to capture temporal trends.
Related Work: 
There are many systems for predicting various diseases that have been proposed and put into use using various approaches and procedures, but our research in this area reveals that not many systems have been established utilizing more than one technology. Tsai and Watanabe used a genetic algorithm m-based method to optimize the fuzzy membership functions in order to classify myocardial heart disease from ultrasonic pictures.[5] Usha Rani's research underscores strategies to improve EEG signal interpretation's reliability and enable accurate predictions, especially for conditions like heart disease.[6] M. A. Jabbar and Shirina Samreen's study presents a notable perspective by introducing the Hidden Naïve Bayes model, demonstrating remarkable promise with a 100% accuracy rate in heart disease prediction.[7] Based on input, S. Sivagowry  used the Artificial Neural Network (ANN) method to categorize the cardiac illness.[8] A prototype model built on the supervised Classification Algorithm is called Learning Vector Quantization (LVQ). Shuge Ouyang's work in the field of medical decision support illustrates the critical trend of using massive clinical data, with a specific focus on cardiac illness prediction. The paper provides an informative review of machine learning approaches by studying diverse cardiac disease types and datasets, emphasizing their versatility while tackling the obstacles given by the diversity of medical data.[9] Siddharth M's work gives a thorough knowledge of Supervised Learning within Machine Learning, analogizing it to the teacher-student relationship. Furthermore, the paper emphasizes K-Nearest Neighbors' varied relevance in heart disease prediction, spanning both classification and regression problems. [10]. 










2.2	Critical Analysis of Existing Studies 
Paper	Limitations	Future scope
[2]	The quality and representativeness of the input data have a significant impact on the accuracy and reliability of prediction models created using data mining techniques. The accuracy of predictions may be impacted if the data used to train the models contains errors, inconsistencies, or biases. Therefore, it is essential to guarantee the accuracy and integrity of the input data.	Additional data sources, such as genetic data, wearable technology, or electronic health records, can be used to gain a deeper understanding of heart disease prediction. The accuracy and robustness of the prediction models can be improved by the integration of these many data sources and the use of data mining techniques.
[3]	When single and hybrid data mining techniques are compared for the detection of heart disease on the CHDD, the results indicate varying degrees of accuracy, with hybrid techniques outperforming single ones.
	This study offers an approach for determining whether applying data mining techniques to the analysis of heart disease treatment data can result in reliable findings that are comparable to those found in heart disease patient diagnosis.
[4]	For training, CNNs and RNNs need a lot of high-quality data. The effectiveness and generalizability of the models may be constrained by the availability and caliber of electronic health records and other pertinent data sources. Prediction accuracy might suffer from biases introduced by incomplete or noisy data.	The development of methods to improve the interpretability and explainability of deep learning models can be the subject of future study. By providing insights into the decision-making process of CNNs and RNNs, tools like attention mechanisms, visualization techniques, or model-agnostic interpretability approaches can help make them more transparent and reliable
[8]	This approach seems to provide only minor improvement of prediction performance

	Studying if pathological data and ensemble reduction could offer even more performance gain is one of our upcoming projects.
Table 1 Critical analysis/ Summary of the existing studies

Chapter 3	
Methodology
The transformation of knowledge bases into decision support systems has paved the way for leveraging knowledge exploration within databases, thereby facilitating the powerful technique of data mining. This process empowers the extraction of pivotal insights from extensive datasets, harnessing a blend of mathematical analysis, advanced machine learning algorithms, and cutting-edge computer technologies. Data mining, in essence, encompasses the intricate art of unearthing valuable, previously concealed information residing within the depths of stored data. This process is underpinned by two fundamental data mining strategies: supervised learning, which involves the use of a structured training set, and unsupervised learning, where this training set is absent.

Translating these foundational principles into the realm of predicting heart disease involves a meticulously orchestrated three-fold approach: Firstly, the identification of a dataset harboring a comprehensive array of 13 therapeutic attributes. Subsequently, a meticulous journey through exploratory data analysis unfolds, intricately refining and preparing the dataset for deeper insights. Lastly, the culmination of this process sees the strategic deployment of sophisticated regression and classification algorithms in the third phase. This judicious application facilitates the meticulous evaluation of diverse models and techniques, thus bolstering the capacity to comprehend intricate heart disease patterns and further paving the path for effective predictive modeling strategies.


 
Figure 1 Methodology

•	Stage 1: Identification of the dataset
At this point, the focus is on finding a dataset that includes data on 13 therapeutic properties. These characteristics could include a variety of elements, including the effectiveness of medical interventions, patient outcomes, or any other pertinent measures. This rigorously assembled dataset is more than simply a collection of data; it is the starting point for all future analysis and inquiries. This dataset, which serves as the foundation of investigation, not only provides a starting point for identifying links and trends contained within the various layers of information, but it also serves as a lens through which the complicated interaction of therapeutic factors may be grasped. The dataset serves as the starting point for any subsequent analysis and serves as a starting point for investigating relationships and trends in the data.

•	Stage 2: Collecting Exploratory Data
The second step entails using the chosen dataset to carry out an extensive exploratory data collection approach. Tasks including data cleansing, addressing missing values, dealing with outliers, and translating the data into an analysis-ready format are frequently included in this step. The dataset must be kept consistent and dependable so that it can be examined further. In order to get insights into the dataset during this stage, several statistical techniques and data visualization techniques may be used. Exploratory data analysis enables researchers to comprehend the properties and distribution of the variables, spot potential correlations or patterns, and produce ideas for additional research.


•	Stage 3: Regression and Classification Analysis
The third stage focuses on employing regression and classification algorithms to evaluate the usefulness of different models and methodologies. Regression analysis aims to establish relationships between independent variables and the 13 therapeutic qualities, allowing for prediction or estimation of the values of these qualities based on other variables. Classification analysis, on the other hand, is employed when the objective is to categorize data into distinct classes or groups based on certain features. Researchers can evaluate the effectiveness and predictive potential of various models and approaches by applying regression and classification algorithms to the dataset. This evaluation sheds light on whether methods are better at identifying underlying patterns and precisely predicting therapeutic features.

3.1	Data Collection and Preprocessing [18] 
•	Study of Dataset:
•	The three basic input features that make up the data are as follows: 
	Objective: to offer empirical data.
	Examining: the outcomes of the medical exam.
	Target Variable:  Which gives the output, that is if the person has disease or no.
o	These input features were subsequently subdivided into a massive dataset that contained all the patient medical examination values. The type of feature for each attribute is described in the table along with its values, units, and categorization code.


Sr.no	Attribute
Name	Types of
 Features	Data 
Type	Categorical
Code
1.	Age	Objective	Int(years)	
2.	Sex	Objective	Binary	0: Female
1: Male
3.	cp
(Chest Pain)	Examination 	Int	0:No pain 1:TypicalAngin
2: Atypical Angina,
3: Non-Anginal Pain
4: Asymptomatic
4.	Trestbps
(Resting Blood Pressure)	Examination	Int(mm Hg)	
5.	Chol
(Cholestrol	Examination	Int	
6.	Fbs
(Fasting Blood Sugar)	Examination
	Binary	If >120 mg/dl
0: No
1: Yes

7.	Restecg
(Resting Electrocardiogram)	Examination	Int	0: Normal
 1: ST-T Wave Abnormality
2: Left Ventricular Hypertrophy
8.	Thalach
(Maximum Heart Rate Achieved)	Examination	Int(Bpm)	
9.	Exang
(Exercise-Induced Angina)	Examination	Binary	0: No
1: Yes
10.	Oldpeak	Examination	Float	ST depression induced by exercise
11.	Slope	Examination	Int	0: Upsloping
1: Flat
 2: Downsloping
12.	Ca	Examination	int	Number of major vessels (0-3) colored by fluorosopy
13.	Thal
(Thallium Stress Test)	Examination	Int	0: normal
1:  fixed defect
2: reversible defect
14.	Target	Target Variable	Binary	0: No Disease
1: Disease
Table 2 DataSet Description

	I have merged 6 data sets which have the same columns and then performed my data preprocessing on the final data set. After merging the 6 datasets, I get a record of 3378 and 14 columns.

	Let us start explaining each attribute of the Dataset:
	Age: The individual’s years of age.
	sex: The individual’s gender (male = 1; female = 0).
	cp: The type of chest pain (0:no pain,1: typical angina, 2:atypical angina, 3:non-anginal pain, 4:asymptomatic) 
	Typical angina: This chest pain is identified by discomfort or pain in the chest that is brought on by decreased blood supply to the heart. Rest or medication can usually ease it. It is typically brought on by physical strain or stress.
	atypical angina: atypical angina is a type of chest pain that resembles typical angina in certain ways but may manifest differently. It could not respond to rest or treatment as predicted, or it might have unexpected triggers.
	non-anginal pain: Chest discomfort that is not caused by decreased blood supply to the heart is referred to as non-anginal pain. It may be brought on by several conditions, including musculoskeletal disorders, respiratory troubles, or digestive disturbances.
	Asymptomatic: Asymptomatic refers to the patient’s lack of heart-related discomfort or chest pain. Despite having underlying heart issues, they might not show any overt symptoms.
	trestbps: The patient’s resting blood pressure (in mm Hg) at the time of hospital admission.
	chol: A person’s cholesterol reading in milligrammes per deciliter.
	fbs: The subject’s fasting blood sugar (more than 120 mg/dl, 1 = yes; 0 = no).
	restecg:Resting electrocardiographic measurement,(0 = normal, 1 = ST-T wave abnormalities, 2 = demonstrating possibly or definitely left ventricular hypertrophy per Estes’ criteria).
	0: Normal – The ECG pattern displays no notable abnormalities and is normal.
	1. ST-T Wave Abnormalities: The ECG shows variations in the ST-T wave, which may signal probable heart problems.
	2: Left Ventricular Hypertrophy – According to Estes’ criteria, the ECG exhibits symptoms of an enlarged left ventricle, which may point to a heart problem.
	Exang: is short for exercise-induced angina.
	thalach: stands for maximal heart rate (1 = yes, 0 = no).
	Oldpeak: This characteristic shows the ST depression brought on by exercise in comparison to rest. Since it is a continuous variable, any positive or negative value may be assigned to it.
	slope: the angle of the ST portion of the peak workout (1: upsloping, 2: flat,3: down sloping,).
	Upsloping: Upsloping: During activity, the ST segment of the ECG displays an upward-sloping pattern that is generally regarded as a normal result.
	Flat: Flat: During activity, the ST segment of the ECG remains largely horizontal, which could signify a less favorable or inconclusive outcome.
	Down sloping: During activity, the ST segment of the ECG displays a downward-sloping pattern, which is regarded as abnormal and may indicate an elevated risk of heart-related problems.
	ca: Major vessel count (between 0 and 3).
	Target: cardiovascular disease (0 = no, 1 = yes).
	thal: Thalassemia is a blood disorder (0 = normal; 1 = fixed abnormality; 2 = reversible defect)
	normal: Normal: There are no noticeable anomalies, and blood flow is normal.
	Fixed Abnormality: Unchanged diminished blood flow, most likely as a result of a past heart problem.
	Reversible Defect: Reduced blood flow during stress that improves after rest, indicating a possible obstruction or ischemia.







•	Data Preprocessing:[11],[12],[13],[14]
•	Data preprocessing, which is a subset of data preparation, refers to any sort of processing carried out on raw data to prepare it for another data processing action. It has traditionally been a crucial first stage in the data mining process. Data preparation approaches have recently been modified in order to execute inferences against AI and machine learning models. An essential phase in the data mining process is data preparation. This step may include feature engineering, which involves creating new variables to encapsulate significant information or connections within the dataset. Data integration may also occur, in which information from many sources is harmoniously combined. After the data has been adjusted and harmonized, it is thoroughly analyzed using a variety of statistical approaches and machine learning algorithms to uncover hidden patterns, correlations, and trends. It explains the steps involved in cleaning, transforming, and combining data in order to prepare it for analysis. Data preprocessing is done to improve the quality and usefulness of the data for data mining.[11]
•	There are 6 types of Data Preprocessing techniques used in this project they are as follows:

	Data Cleaning: Data cleaning entails handling any missing values, getting rid of any values that are duplicates, and addressing any conflicts in the dataset. To fill up the gaps created by missing data, imputation techniques like mean imputation, median imputation, or more complex methods like multiple imputation can be utilized. Finding and removing duplicates is one way to guarantee data integrity. Multiple imputation, mean imputation, and other sophisticated approaches like multiple imputation can all be used to impute missing values [12]. 
	Feature Scaling: To put the scales and units of the dataset’s variables into a similar range, it is frequently desirable to utilize feature scaling. Two popular scaling methods, normalization (min-max scaling) and standardization (mean normalization), ensure that all variables are scaled uniformly and prevent any one variable from gaining too much weight in the analysis due to its larger magnitude.
•	Feature Encoding: To be used for analysis, categorical variables like sex, cp, fbs, restecg, exang, and thal must be encoded to numerical values. One-hot encoding or label encoding, in which each category is represented by a binary variable, are two techniques that can be used to accomplish this.
•	Handling Outliers: Noisy data, or outliers, cannot be comprehended by machines and has no meaning. It might have been made as a result of inaccurate data entry, bad data collection, etc. [11] This study could be affected by any outliers in the data. Outliers can have a significant impact on the study's results. As a strategic reaction, it is critical to perform a detailed examination of these outliers, digging into their nature and determining their impact. By analyzing the nature and impact of the outliers, decide whether to get rid of them or change them using the appropriate techniques, such as utilizing a standard scaler to balance the data.
•	Feature Selection: Feature¬ selection aims to identify the¬ most crucial variables, thereby re¬ducing dimensionality and boosting model performance¬. Various methods can be employe¬d for this purpose, including correlation analysis, Chi Square te¬st, backward, or forward feature sele¬ction techniques, as well as re¬gularization approaches like Lasso or Ridge re¬gression.
•	Data Splitting: In machine le¬arning, a crucial technique involves cre-ating training and testing sets from the datase¬t. The primary goal of this step is to establish re¬normalize scenarios where the¬ model encounters pre¬viously unseen data. By evaluating the¬ model’s performance on the¬ testing set, we can gauge¬ its ability to generalize and pre¬dict how well it will perform on fresh, unte¬sted instances. Hasty et al. (2009) sugge¬st common splitting ratios of 70-30 or 80-20, which allocate 70% or 80% of the data for training purposes and the¬ remaining 30% or 20% for testing purposes. This division e¬nsures an adequate amount of data for both training the¬ model and evaluating its performance¬.[14] To maintain the inte¬grity of model training, it is crucial to ensure a random distribution of occurre¬nces in both the training and testing se¬ts. This helps prevent bias or ove¬rfitting. Properly dividing the data is esse¬ntial for accurately assessing the mode¬l’s performance and obtaining objective¬ performance indicators.

3.2	ML/AI Model Development
This section directs us by providing a brief description of each of the various data mining algorithms used, along with information on how they operate and the benefits and drawbacks of utilizing them in the development of a decision support system.


3.2.1	Logistic Regression [15]
Logistic regre¬ssion [LR] is a widely used and depe¬ndable technique for binary classification proble¬ms. It proves particularly valuable when pre¬dicting the likelihood of an eve¬nt or outcome falling into a specific category. In custome¬r behavior analysis, logistic regression addre¬sses various challenges, such as fore¬casting whether a customer will make¬ a purchase or switch to a competitor. Additionally, it helps de¬termine the probability of use¬rs clicking on advertisement links and inve¬stigate other binary classification issues. The ease of use and interpretability of logistic regression is one of its benefits. It is a popular option for initial machine learning jobs because it is simple to use and comprehend. Furthermore, logistic regression can shed light on the connection between independent variables and the likelihood of the dependent binary variable, enabling the discovery of key variables that significantly affect the outcome.

The log-odds of the dependent variable and the independent variables are assumed to have a linear connection by logistic regression, which is a useful tool but must be understood. More sophisticated techniques, such deep learning, can be considered when the relationship is non-linear or more complex. Deep learning techniques can perform better in tasks that call for the modelling of highly non-linear dependencies due to their capacity to grasp complicated patterns and correlations.



	How it Works:
o	A mathematical technique for estimating binary classes is logistic regression. In essence, the effect or aim characteristic is dichotomous.
o	Dichotomous suggests that there are just two groupings accessible. For instance, it can be applied to cancer identification issues.
o	Calculates the probability of an event.
o	The categorical focus variable is an exception to the general rule of linear regression.
o	Bases the variance on an odds log. The logistic regression calculates the probability of a binary event using a logit function.

	Advantages:
	Because of its efficiency and transparency, it does not require a lot of processing power and is easy to apply.
	It is simple to grasp, frequently used by scientists and data analysts, and does not include function scaling.
	Disadvantages:
	Multiple categorical characteristics or variables cannot be accommodated by logistic regression.
	Transformation of non-linear characteristics is crucial since logistic regression cannot handle the non-linear problem and is overfit-prone.
	Logistic regression would be ineffective for independent variables that are not connected to and contrasted with the target variable.

3.2.2	Decision tree [16,17]
Each leaf node in a decision tree [DT] symbolizes a conclusion, and each interior node a feature or attribute of the data. It is a tree-like structure that resembles a flowchart. The topmost node in a decision tree, known as the root node, can partition data so that it can be divided based on various attribute values. Recursively, the tree structure is constructed, separating the data into subsets based on the attribute values at each internal node. It is possible to implement a variety of decision-making strategies thanks to this adaptable diagram structure. A graphic depiction of reasoning that closely resembles human thought is the decision tree. Because it offers openness and interpretability, the method is frequently referred to as a “white box” machine learning algorithm. In contrast to “black box” algorithms like neural networks, decision trees provide a transparent and intelligible justification for making judgements. Users can analyze the decision-making process and comprehend the elements that contribute to the outcomes thanks to this transparency. A non-parametric or variance-free method that does not rely on the variance of probability assumptions is the decision tree. High-dimensional data can be accommodated by decision trees with exceptional precision.
Decision trees are strong and adaptable machine learning models that have many applications across a variety of areas. They are very beneficial for projects like classification and regression. A decision tree’s structure is made up of leaves that represent the ultimate projected classes or values and branches that indicate various decisions or outcomes based on attribute values. The decision tree algorithm determines the best characteristic and value for separating the data at each internal node using several criteria, such as Gini impurity or information gain.

 
Figure 2 Example of decision tree


	How it works:
	Apply Attribute Selection Steps (ASM) to select the proper attribute and set new records.
	Draw the judgement node element, then divide the dataset into more manageable portions.
	Before one of the prerequisites is met, the tree construction process begins by iteratively repeating this procedure for each child:
	The magnitude of the attribute is the same for both tuples.
	No more attributes are present.
	Cases no longer exist.
 
Figure 3 Decision Tree

	Advantages
	Decision trees can be readily depicted and viewed.
	Quickly recognize non-linear patterns.
	Less pre-processing of the data by the user is required; for instance, columns do not need to be standardized.
	Applicable for variable collecting and used in practical engineering, including calculating lost values.
	Because the approach is non-parametric, the decision tree lacks distribution characteristics.

	Disadvantages
	Data vulnerable to noise. It may overshadow obvious outcomes.
	The slight variance (or fluctuation) in the data may influence the decision-making process in a different way. This will be lessened by algorithms for boosting and bagging.
	Because the difference between the data sets affects decision trees in some ways, it is recommended that the datasets be matched before the decision tree is constructed.
3.2.3	Random Forest Classifier [18]
A well-liked and effective supervised learning approach called random forests[RFC] can be applied to both classification and regression applications. They are renowned for their adaptation to different problem domains, user-friendliness, and versatility. The term “random forests” refers to a group of decision trees that work together to build a forest, which gets stronger as the number of trees grows. For random forests to function, randomly chosen data subsets are used to build numerous decision trees. The final prediction is made by aggregating the outputs from all the decision trees, either through voting (for classification) or averaging (for regression), and each decision tree in the forest separately provides predictions. This ensemble method enhances the model’s overall robustness and accuracy while reducing overfitting. Random forests have the advantage of being able to indicate the relevance of a feature. Random forests can evaluate the usefulness and contribution of each feature in creating predictions by looking at the performance of the trees. When performing processes like feature selection, where one needs to pinpoint the variables that are most useful for the issue at hand, this information on feature importance might be helpful.

Random forests have a wide range of uses across numerous industries. In projects including picture classification, recommendation engines, fraud detection, and loan candidate categorization, they have been employed successfully. Random forests can also be used in the healthcare industry to predict diseases or spot patterns that point to specific ailments. Additionally, they have served as the basis for the Boruta algorithm, a feature selection method that locates crucial features in datasets. Random forests are a flexible and effective supervised learning system, to sum up. They handle both classification and regression tasks and increase prediction accuracy by utilizing the strengths of several decision trees. Random forests continue to be a preferred option for machine learning practitioners due to its feature importance analysis and broad range of applications.
	How it works:
o	There are 4 important steps
o	Draw samples at random from a single dataset.
o	Create a decision tree for each question, then use each decision tree to anticipate the answer.
o	Conduct a vote for each predicted result.
o	Assign the prediction outcome with the most votes as the final forecast.

 
Figure 4 Working of Random Forest

	Advantages:
	No matter how many decision trees are utilized in the process, random forests are extremely predictive and robust.
	The problem of overfitting is not present. The main justification is that all forecasts must be added together in order to eliminate forecasts.
	The algorithm can be applied to classification and regression issues.
	Missing values can also be handled using random forests. These can be done in one of two ways: by substituting median values for constant variables, or by computing the proximity-weighted average in the absence of values.
	You can determine the relative importance of features to help you choose the classification’s most crucial elements.



	Disadvantages:
	Since they contain a lot of decision trees, random forests take a while to provide forecasts.
	Every time a projection is created, every tree in the forest must first estimate the same feedback before deciding. It’s a long process.
	The model is more difficult to understand than the logic tree because you can just choose by going down the tree’s path

3.2.4	K Nearest Neighbor[19]
The k-Nearest Neighbours (KNN) algorithm is a popular supervised learning technique that may be used for both classification and regression issues. KNN is renowned for its brevity and straightforward methodology. The idea behind it is that similar instances or data points typically have comparable labels or values. In KNN, the parameter “k” stands for the number of nearest neighbours that will be considered when producing predictions. It calculates the "k" nearest neighbours in the training data given a new instance using a distance metric such as Euclidean distance. The forecast is then formed by casting a majority vote among those closest neighbours' labels or values (for classification) or averaging them (for regression). The KNN method has the advantages of being simple to implement and interpret. Since it uses the complete training dataset to make predictions, there is no need for a training phase. KNN is ideal for a variety of applications since it can handle both category and numerical input. However, the distance measure and “k” value choices can have an impact on how well KNN performs. The curse of dimensionality can have an influence on an algorithm in high-dimensional domains. To make sure that all features contribute equally to the distance calculation, feature scaling is frequently advised.
KNN has been effectively used in a few fields, including anomaly detection, recommendation systems, and image recognition. It is especially helpful when there is no underlying assumption on the distribution of the data or when the decision limits are not linear. Combining KNN with other strategies, like weighted distance metrics or dimensionality reduction methods, can boost its performance. 
In conclusion, KNN is a simple and clear algorithm that creates predictions based on the concept of similarity. It can be used for a variety of classification and regression challenges because to its versatility and ease of implementation. KNN is still a useful technique in machine learning because it offers interpretability and application in a variety of domains, despite being sensitive to parameter choices and the curse of dimensionality.

 
Figure 5 Example of KNN

	How it Works:
	Given a new instance that needs to be classified, KNN determines the k closest neighbors in the training dataset using a selected distance metric (such as Euclidean distance). These neighbors are the training set data points that, in terms of feature similarity, are most comparable to the new instance.
	For classification tasks, KNN polls the k closest neighbors to choose the new instance’s class label. The class with the most votes is chosen as the anticipated class label for the new instance. Each neighbor’s class label contributes one vote.
	Regression Tasks: KNN computes the average of the target values of the k nearest neighbors for regression tasks. The anticipated value for the new instance is given as this average value.
	Handling Ties: Several strategies can be employed when there is a tie in the voting process (i.e., when different classes earn the same number of votes). One typical strategy is to priorities the closest neighbors based on how close they are to the new instance.
	repeat these processes for every new instance. KNN uses the knowledge offered by its k nearest neighbors to categories or predict the values of unknown data items.

	Advantages:
	Simple and intuitive: KNN is a good option for novices and for rapid prototyping because it is simple to learn and apply.
	KNN uses the entire training dataset as its model, hence it does not require an explicit training phase. This makes it useful in instances involving online learning.
	Non-Parametric: KNN does not make any assumptions about the distribution of the underlying data, which enables it to be used for a variety of issues, including both linear and non-linear relationships.
	Flexibility in Decision Boundaries: Because KNN considers the local surroundings of data points rather than depending on generalizations, it can capture complicated decision boundaries.
	Versatility: KNN is a versatile approach for a range of supervised learning issues since it can handle both classification and regression tasks.

	Disadvantages:
	Computing Complexity: As the training dataset gets larger, KNN becomes more computationally expensive since it must calculate distances between each new instance and every previous occurrence.
	KNN is susceptible to noisy or irrelevant features as well as outliers, which might affect the accuracy of predictions.
	The performance of KNN suffers from the “curse of dimensionality” as there are more features or dimensions. The data points grow sparser and the effectiveness of distance-based calculations decreases in high-dimensional spaces.
	Need for Optimal K Selection: The performance of the model can be greatly impacted by the parameter k (number of neighbors). Choosing the wrong value for k might result in underfitting or overfitting.
	Data that is Unbalanced: In unbalanced datasets, KNN tends to favor the dominant class. Predictions may be biased in favor of the dominant class when there is an imbalance between the classes.


3.3	Evaluation of the Proposed System
In order to measure the effectiveness of the AI-based models created for heart disease prediction, the proposed system will be evaluated using recognized assessment metrics, such as accuracy, sensitivity, specificity, and F1 score. The performance of the models in accurately classifying examples is assessed using these measures, which are frequently employed in classification tasks.

	According to Japkowicz and Shah (2011), accuracy quantifies how accurate a model’s predictions are on average by measuring the proportion of accurate forecasts to all instances. It offers a broad evaluation of the model’s accuracy in predicting both positive and negative events.[20]

	According to Powers (2011), sensitivity, sometimes referred to as the true positive rate or recall, is a metric that indicates how well a model can detect people who have heart disease. Out of all actual positive cases, it shows the percentage of true positive cases that the model properly recognized. The model’s high sensitivity suggests that it is efficient in identifying people with heart disease and reducing false negatives.[21]


	According to Powers (2011), specificity assesses how well a model can categorize people who do not have cardiac disease. Out of all actual negative situations, it shows the percentage of true negative cases that the model properly detected. The model is effective at avoiding false positives and correctly identifying people without heart disease when its specificity is high.


	According to Saito and Rehmsmeier (2015), the F1 score is a composite metric that combines recall and precision. Recall is the percentage of real positive cases detected by the model out of all actual positive cases, whereas precision reflects the proportion of true positive cases out of the instances predicted as positive. By considering both precision and recall, the F1 score offers a fair assessment of the model’s performance. Given that it considers the trade-off between minimizing false positives and false negatives, it is especially helpful when the data is unbalanced.[22]


	The evaluation of classification models, especially those employed to forecast cardiac disease, has frequently utilized these evaluation metrics. They offer a thorough evaluation of the model’s performance in terms of precision, recall, sensitivity, and the right amount of each. Researchers can learn more about the models’ efficacy, spot opportunities for development, and make defensible choices regarding model optimization or selection by evaluating the models using these measures.




Chapter 4	
Experimental Results
4.1	Experimental Setup
In this chapter, we will discuss how the data was pre-processed, what techniques were used to clean the data, which assumptions were made for enhanced data efficiency, and the overall implementation of the EDA and its components. There are, in my opinion, fundamental components of data exploration.
We shall look at the following elements in this document:
	Recognize your variables.
	Cleaning up your data.
	Relationship analysis between variables.
4.1.1	Hardware Setup
	Processor (CPU): Intel Core i7 or i5 (or AMD equivalent) with many cores for greater gaming and multitasking performance.

	NVIDIA GeForce GTX or RTX series graphics cards or AMD Radeon graphics cards are optimised for gaming and graphics-intensive workloads.

	Memory (RAM): 8GB or 16GB DDR4 RAM is typical, providing seamless performance while running many applications and games.

	Storage: Typically includes a Solid-State Drive (SSD) for faster boot times and application loading, as well as an extra Hard Disc Drive (HDD) for increased storage capacity.




4.1.2	Software Setup
	Operating system: Windows 11 Home Single Language. Version: 22H2.
	Drivers: HP-supplied drivers for hardware components enable appropriate operation.
	Google colab: The whole code is written on google colab using python.
	Data Analysis Libraries: To handle and analyse the dataset, popular data analysis libraries such as Pandas, NumPy, Matplotlib are used.
	Machine Learning Libraries: For constructing and training predictive models, we used machine learning libraries such as Scikit-learn, TensorFlow.
	Data Visualisation: To visualise data distributions and patterns, I used data visualisation libraries such as Matplotlib or Seaborn.

4.1.3	Exploratory Data Analysis [23]
Exploratory Data Analysis (EDA) constitutes a vital stage within the Data Analysis Process, dedicated to gaining comprehensive insights into a dataset. This multifaceted approach involves a variety of techniques aimed at unraveling the intricacies of the data at hand. It encompasses the discernment of pertinent variables while filtering out inconsequential ones, the identification and resolution of anomalies like outliers, missing data, and errors, as well as the exploration of interrelationships between different variables. This analytical journey begins with an inquisitive lens as we attempt to decipher the underlying structure, patterns, and trends buried in the data. EDA is a key to comprehending the subtleties of the therapeutic characteristics under inquiry. EDA also serves the purpose of maximizing the utility of the dataset, extracting profound insights while minimizing the potential for errors in subsequent analyses.
The oft-quoted adage “garbage in, garbage out” underscores the significance of data quality, highlighting that flawed input leads to flawed output. In contrast, EDA embodies a modified perspective: “garbage in, EDA performed, possibly garbage out.” This indicates that the application of EDA has the capacity to elevate a partially usable dataset to a realm of full usability, but it’s not a panacea for all data-related woes.
While EDA is not a panacea for all data-related challenges, it does offer an arsenal of techniques to tackle prevalent issues encountered across a spectrum of datasets. In essence, EDA empowers data analysts to unearth latent patterns, correlations, and potential pitfalls, thereby refining the dataset and laying the groundwork for more insightful and precise analyses.
a.	Understanding your Variables
You have no idea what you do not know, what you do not realise. And how do you know if your thoughts are meaningful if you do not know what you do not know?
• First, I imported all the libraries I believed I would like to analyse and ran some preliminary analysis.
 
Figure 6 Loading Libraries





•	Secondly, I moved all the six data sets to my google drive so it is easy to load the data set.


 
Figure 8 Second Data Set
•	I had to change the condition column to target column so it matches all the data sets. I did that by using the rename () function:
 
Figure 9 Changed condition to target

 
Figure 10 Third Data Set
 

Figure 11 Fourth Data Set
 

Figure 12 Fifth Data Set
 

 

Figure 13 Sixth Data Set

•	In the Sixth Data set we can see that cleaning is required for the sex and target column. So, for that I used the mapping function.


 
Figure 14 Mapped sex and target columns

•	After being acquainted with all the dataset variables, I sought deeper insights into their various ramifications. I generated a complete summary by using the “describe ()” function, which included vital data such as counts, averages, standard deviations, and the range extending from minimum to highest numeric values for each variable. This strategy improved my comprehension of the dataset’s intricate properties.
 

Figure 15 Summary of the First Dataset


 

Figure 16 Summary of the Second Data Set
 

Figure 17 Summary of the Third Data Set

 

Figure 18 Summary of the Fourth Data Set

 
Figure 19 Summary of the Fifth Data Set


 

Figure 20 Summary of the Sixth Data Set

•	After that I merged all the 6 data sets so that I could have one large dataset and I could work on it

 

Figure 21 Merged Data Set

•	Used isNull() and dropna() function to find and fill the null values in the Data Set.



 
Figure 22 Finding out null values and dropping null values in my dataset 





 

Figure 23 Summary of the Merged Data Set


•	Info () function is used method displays a brief summary of a Data Frame, including relevant information about its structure, data types, memory utilisation, and non-null values.
 
Figure 24 Used info() to get information of the table


•	Shape (): That returns the number of rows to my data set by the number of columns. My output was (3378, 14) that is 3378rows and 14 columns in the dataset.
•	columns():Returns the name of all dataset columns.


 

Figure 25 Shape and Columns function











•	Unique () function is used to print the unique values of the respected column

 
Figure 26 Printing unique values of each column(1)






 
Figure 27 Printing unique values of each column(2)


 
Figure 28 Printing unique values of each column(3)

 
Figure 29 Printing unique values of each column(4)


b.	Variables
•	The term “variables” refers to a property of an item that has a changing value that is reliant on other factors. Based on the variables, you wish to measure, you can choose a solid research design for the study.
	For instance: if you want to assess a country’s GDP status, you must evaluate the elements that affect the country’s GDP. Individual income, market structure, product demand, and supply statistics for the economy’s goods and services are examples. As a result, as these components change, so does the GDP of the economy. As a result, variables are defined as factors with moving values that influence other factors.
•	Types of Data:
	The value of a certain variable that you have entered in the datasheet for usage is referred to as data. Data theses are divided into two types:
	Data theses are classified into two types:
•	Quantitative data that represents a real or numerical value.
•	Categorical data entails grouping and clustering within a group.

	Numeric Variables: To build value from quantitative variables, you must collect real numbers and add, subtract, multiply, or divide them. There are two types of quantitative variables: discrete and continuous.
	Discrete Variables: Discrete Variables: These variables have clear, distinct values and gaps between them. The number of children in a family or the number of cars in a parking lot are two examples.
	Continuous Variables: Continuous variables can take on any value within a range and have an endless number of possible values. Height, weight, and temperature are a few examples. These variables are often measured using various precision instruments, allowing for a continuous range of values.
	Discrete vs Continuous:
Variables	Definitions 	Example
Discrete	Finite	Number of shoes in a showroom
Number of students in the class
continuous	Non-Finite	Volume, Weight,
Age, Distance 
Table 3 Discrete vs Continuous Variable

	Categorical Variables: Categorical variables are groups of some types of categories that must be recorded and numerically represented. However, the numeric reveals the specific categories rather than the variable values. Categorical variables are classified into three types:
•	Binary. 
•	Nominal.
•	Ordinal Variables.
Variables	Definition	Example
Binary 	Refers to whether the outcome is positive or bad and is coded as two numbers	•	Student Status
•	Yes/No
•	1 or 0
Nominal 	Refers to groups that have no ranks inside them.	•	Gender
•	Eye colors
•	Blood type
•	Orgin
Ordinal 	It is the group, which is prioritized based on a specified number basis	•	Rating performance
•	Pain intensity
•	Ranking of classes
Table 4 Categorical Variables


1.	Analyzing relationships between variables:
•	Correlation Matrix: When embarking on variable analysis, my initial preference lies in constructing a correlation matrix, as it provides a straightforward avenue for gaining a holistic understanding of the variables at hand. This method is particularly adept at revealing associations between different variables. The process involves calculating correlation coefficients to evaluate these interrelationships. Essentially, a correlation matrix takes the form of a table, presenting the correlation coefficients among multiple variables. My chosen tool for this endeavor is the "sns.heatmap()" function, which facilitates the creation of an illustrative correlation matrix, thereby aiding in the identification of patterns and connections within the dataset.
 
Figure 30 Process of the code for correlation matrix

 
Figure 31 Output of the above code

•	After plotting the graph, I wanted to see the correlation coefficient with the target column.


 
Figure 32 Correlation Coefficient

•	Histogram: This plot can be used to quickly visualize the distribution of data in the Data Frame, detect potential outliers, understand the spread and central tendency of the variables, and obtain insights into the overall form of the data in each column. The function to plot this code is “hist ().”
 
Figure 33 Histogram (1)

 
Figure 34 Histogram (2)

•	Let us first examine our target variable:
I assessed dataset balance by creating a bar graph depicting the distribution of the target variable in relation to heart disease prediction. The visualization revealed that 51.42% of the persons in the dataset were classified as not having heart disease, while a little lower number, 48.58%, were classified as being at risk of acquiring heart disease. This nearly equal distribution of the two categories reveals a favorable equilibrium within the dataset, offering a solid platform for future analytical endeavors and confirming our ability to proceed with our inquiry.
 
Figure 35 The balanced between the Data set

 
Figure 36 percentage of the target attribute

•	Using the property “age”:
o	The provided code generates two compelling visualizations, each contributing distinct insights to the analysis.
o	The initial graph presents an insightful Age Distribution analysis in relation to the target variable. This visual depiction effectively assists in identifying potential connections between age and the propensity to develop heart disease.
o	The subsequent graph, building upon the foundation of the first, delves deeper into the data by showcasing disease counts categorized by both gender and age. This extended analysis unveils how gender intertwines with age and impacts the incidence of heart disease.
o	Worth noting is the strategic focus of the second graph on individuals who exhibit the condition (“cardio = 1”). By concentrating on this subset, the visualization aims to facilitate the anticipation of age and gender patterns among those affected, offering a more targeted and insightful perspective.
o	The graph below shows that those over the age of 52 are more likely to have diseased than those under the age of 54. And according to our findings, men aged 50 and up appear to be more likely than women to have been diagnosed with heart disease. While this result confirms our initial hypothesis, it is worth noting that the observed difference in prevalence, while suggestive, does not reach statistical significance. Despite the lack of statistical significance, this discovery sheds some light on the potential association between age and heart disease in our sample. More research and potentially larger sample sizes could help validate and quantify this observation.
 
Figure 37 Working on the ‘age’ attribute

•	Using the property ‘sex’:
o	Firstly, I plotted the graph which visually shows us difference between the genders.
o	The second code tells the exact percentage of males and females.
o	28.51% females and 71.49% males in total

 
Figure 38 Working on the ‘sex’ attribute

o	Here we can see that males are more likely to have heart diseases as compared to females. There are many reasons for this. For instance, one of them is, Biological Factor and Hormonal Influence.
	Biological Factor: Men often have higher levels of substances(chol, fbs etc) that can increase the chances of heart disease.
	Hormonal Influence: Women have a natural defense against heart disease that weakens with age, but males do not.
 
Figure 39 ‘sex’ vs ‘target’

o	Opting for the “sns.countplot” function is the most suitable approach given that our attribute “sex” is categorical. This choice allows us to create a visual histogram-like representation tailored for categorical values. Notably, the advantage lies in the compatibility of its API and settings with the standard bar plot, ensuring a seamless comparison of counts across nested variables.
o	The application of “sns.pointplot” serves as a valuable tool for analyzing the “target” attribute, indicating the presence of disease. By plotting the average of the central trend through scatter points, this visualization method effectively captures essential insights. Furthermore, the incorporation of error bars provides a valuable glimpse into the inherent prediction ambiguity associated with the dataset, enriching the overall analysis. Another way to plot this visualization is by using “pd.crosstab”.
o	This Pandas function does a cross-tabulation (also known as a contingency table) between two categorical variables:’sex’ and ‘target’ (which most likely indicates the presence or absence of heart disease). It essentially counts the occurrences of each’sex’ and ‘target’ combination.

 
Figure 40 A graphical representation of “Heart Disease Frequency for Sex”





•	Using the Attribute ‘Fbs’:
o	 Same as the sex attribute, pd.crossfit is used to plot and visualize the fbs column.
o	As the fbs rises above 120 mg/dl, we witness an increase in the number of persons suffering from heart disease.


 
Figure 41 “Plotting the Fbs Attribute”






•	Using the Attribute ‘exang’:
o	Pd.crossfit is used here to plot this attribute.
o	When you are exercising and you feel a pain in your chest its likely to be a heart disease. That’s what Exercise-Induced Angina means.
 
Figure 42 “Plotting the exang Attribute”












•	Working with ‘cp’ attribute:
o	Crossfit is used in the first graph as we get a very clear picture about the chest pain.
o	distplot(): the code visually analyzes the relationship between chest pain and heart disease by creating two subplots. The first subplot compares the distribution of chest pain levels for instances with and without heart disease. The second subplot illustrates how disease counts are distributed across various levels of chest pain.
 
Figure 43 Plotting the cp(1)



 
Figure 44 Plotting the cp(2)















•	Working with ‘restecg’ attribute:
o	From the below graph we can see that higher restecg levels frequently suggest abnormal heart electrical activity and probable blood flow difficulties, which are linked to disorders like as arrhythmias and ischemia, increasing the risk of heart disease.
 
Figure 45 Working with “restecg”






•	Working with ‘slope’ attribute:
o	CrossFit is used to plot the slope attribute as well.
o	Two main reasons by which we can say that slope is directly proportional to heart disease are as follows:
o	Less Blood Flow Warning: When the ST line goes up on a heart test, it might mean not enough blood is reaching the heart, showing a risk for heart problems.
o	Heart Damage Sign: If the ST line is higher, it could show that the heart is hurt, making the chances of heart issues more serious.

 
Figure 46 Working with “slope”


•	Working with ‘ca’ attribute:
o	CrossFit is used to plot “ca vs target. ’ca’ is the Number of major vessels (0-3) colored by fluoroscopy.
o	The number of colorful blood arteries seen during tests indicates how obstructed they are, informs therapies, and helps doctors determine the severity of cardiac problems.

 
Figure 47 Working with “ca”






•	Working with ‘thal’ attribute:
o	I used the provided code to construct a visualization that combines a count plot demonstrating the distribution of ‘thal’ values in my dataset with a sprite animation. I also included a twin point plot to show the proportion of ‘diseased’ cases in each ‘thal’ group.

 
Figure 48 Working with thal”






•	Working with ‘oldpeak’ attribute:
o	I created a ridge plot in Seaborn to show how ‘oldpeak’ values correlate with the occurrence of heart disease in our dataset. I have shown the distribution of ‘Old Peak’ on the x-axis and ‘Density’ on the y-axis using many layers of histograms and kernel density estimates. This visualization sheds light on the possible link between ‘Old Peak’ and the occurrence of heart disease.

 
Figure 49 Working with “Oldpeak”


•	Working with ‘Cholesterol attribute:
o	I have created the ridge plot for cholesterol as well.
o	Cholesterol is essential for heart health. Elevated LDL cholesterol levels can cause plaque to build up in arteries, reducing blood flow and increasing the risk of heart disease. Monitoring and maintaining cholesterol levels is critical for lowering this risk and promoting heart health.



 
Figure 50 working with “Cholesterol”





•	Working with ‘Thalach attribute:
o	I used Seaborn to generate a ridge plot that shows the distribution of maximal heart rates reached (‘thalach’) in relation to the presence or absence of heart disease (‘target’) in our dataset, ‘df1’. The visualization assists me in determining whether there is a detectable difference in heart rates between the two groups, potentially suggesting a link between maximal heart rate and risk of heart disease. This knowledge could be useful for identifying high-risk individuals and tailoring personalized health advice based on their exercise stress test results.
 
Figure 51 Working with “Thalach”
•	Assumptions:
o	Age is a crucial predictor of cardiovascular health and is assumed to be directly related to the risk of heart disease, with older persons being more vulnerable.
o	Given that biological variations may affect the onset and appearance of the disease, and that males and females may exhibit different risk patterns, it is considered that gender has a significant impact on heart disease. We can observe that men are more likely than women to be diagnosed with heart disease.
o	Chest pain (cp) is regarded to be a major symptom and, if related to heart disease, its presence or form is thought to be a marker of a higher risk.
o	Elevated levels of resting blood pressure (Trestbps) may indicate an increased risk of heart disease, which is assumed to have a direct relationship with the condition.
o	It is assumed that both high and low cholesterol levels, which result in an unfavorable LDL to HDL ratio, increase the risk of heart disease. As a result, maintaining a good balance of LDL and HDL cholesterol is critical for lowering the risk of cardiovascular disease.
o	Fasting blood sugar is thought to influence the risk of heart disease, with higher levels potentially indicating poor glucose metabolism and increased cardiovascular risk.
o	The resting electrocardiogram is anticipated to offer information about heart function by emphasizing aberrant patterns associated with cardiac illness.
o	Maximum Heart Rate Achieved is thought to be significant, with lower or higher rates potentially signaling cardiovascular health concerns.
o	Exang (Exertional-Induced Angina) is a significant predictor of heart disease, with its presence potentially indicating lower blood flow during physical activity.
o	Oldpeak, Slope, and the number of main vessels discovered by the Thallium Stress Test (Ca and Thal) are thought to be essential aspects because they can show the degree and extent of coronary artery disease, which influences heart disease prediction.

•	Assumptions for Feature Engineering Steps:
o	I started by importing the relevant libraries, such as ‘pandas’ for data processing and ‘SelectKBest’ for feature selection, into this code. Then, using our DataFrame ‘df1’, I defined the input features (X) and the target variable (y). I then used ‘MinMaxScaler’ to scale the data to ensure that all features were on the same scale. Finally, I used the chi-square test to choose the top five factors that had the greatest impact on predicting the ‘target’ variable.
 
Figure 52 Choosing the best 5 features
o	To divide the data into training and testing sets, I used the ‘train_test_split’ function from’sklearn.model_selection’. I discovered that the ‘merged’ DataFrame has a total of ‘num_rows’ rows by analysing the ‘cp’, ‘fbs’,’restecg’,’slope’, and ‘ca’ columns. I acquired ‘tr.shape[0]’ samples in the training set (‘tr’) and ‘te.shape[0]’ samples in the testing set (‘te’) after setting a test size of 20% and a random state of 2000.


 
Figure 53 Train and Test







o	Then, using the ‘StandardScaler’ command, I applied standard scaling to verify that all features were on the same scale.
o	StandardScaler()  ormalizes characteristics by subtracting their mean and dividing by their standard deviation. This procedure assures that all features have a mean of zero and a standard deviation of one, bringing them all to the same scale. Many machine learning methods rely on this step to reduce bias caused by feature scaling.
 
Figure 54 Using Standard Scalar



4.2	Results
4.2.1	Training and Testing
•	To assure the accuracy of our model’s predictions, I divided the dataset into discrete training and evaluation subsets. This separation serves an important purpose: by allowing the model to learn from the training data and then evaluating its performance on separate evaluation data, we can simulate how well the model will generalise to new, previously unseen data points.

•	Throughout this procedure, we keep wary of two major hazards that could jeopardise the model’s reliability. For starters, overfitting—when the model becomes overly customised to the training data—can result in a loss of generalisation capability. Second, excessive alterations to the model based on its performance on evaluation data may result in instability and loss of predictive capabilities.


•	To ensure the model’s integrity, we rigorously distinguish between training and evaluation data. The training dataset comprises values that are well-known, allowing the model to discover patterns and correlations. Following that, the model applies what it has learned to the assessment dataset. To assess the model’s performance objectively, we create a reference dataset that represents a subset or category of data. This provides as a standard against which we may evaluate the predictive accuracy of our model.


 
Figure 55 Training and testing the Data Set




4.2.2	Logistic Regression

•	Accuracy and Prediction:
 
Figure 56 Logistic Regression


•	After getting the prediction I wanted to show the confusion matrix, f1 score and Specificity so I did the following:
 
Figure 57 Confusion matrix
•	A confusion matrix is a table used to assess the effectiveness of a classification model. It displays the number of true positives, true negatives, false positives, and false negatives.

 
Figure 58 F1 score

•	F1 score = 0.751.

 
Figure 59 Specificity
•	Specificity = 0.728.






•	After getting the f1 score and specificity I wanted to plot the confusion matrix.
 
Figure 60 Plotting the confusion matrix






4.2.3	Decision tree
•	Accuracy for Decision tree:

 

Figure 61 Decision tree

•	After this I tried using the GridSearchCV module and I found out that I could find the best parameters to tune my decision tree.
 

Figure 62 Using GridSearchCv to tune my decision tree(1)

 
Figure 63 Using GridSearchCv to tune my decision tree(2)

o	GridSearchCv: 
o	I have used the GridSearchCV module from the sklearn package to fine-tune a Decision Tree Classifier in this code. I have made a parameter grid called param_grid that has potential values for hyperparameters like criteria, max_depth, min_samples_split, and min_samples_leaf. I am systematically examining different combinations of these hyperparameters using 5-fold cross-validation on my training data to find the best arrangement. When I finish fitting, I will have access to the best parameters and their matching cross-validation score. The main goal is to improve the performance of my Decision Tree model by choosing the most effective hyperparameter combinations.
o	Best Params(): refers to the ideal configuration of hyperparameters for a machine learning model, which results in optimal performance.
o	Best score(): denotes the model’s highest assessment measure, demonstrating its performance with the selected hyperparameters.

•	When I use the same Parameters, I get a greater accuracy in my decision tree. 
Figure 64 Decision Tree with (parameters)

•	The parameters for decision tree are as follows:
o	Criteria() : specifies the criterion for evaluating quality while splitting nodes in a decision tree.
o	max_depth(): determines the maximum depth of a decision tree, hence limiting its level of branching.
o	Min_samples_split (): calculates the minimal number of samples required to split a node in a decision tree.
o	Min_samples_leaf (): sets the minimal amount of samples that should be present in a decision tree’s leaf node.


4.2.4	K Nearest Neighbor

•	I’m using the sklearn package to create a machine learning model. I am specifically interested in discovering the best “k” parameter for a K-Nearest Neighbours (KNN) algorithm. Consider this as deciding how many of your closest friends’ opinions to consider while deciding. I am experimenting with various “k” values ranging from 1 to 14. For each “k,” I create a unique KNN model.
 
Figure 65 Code for KNN

•	I obtain accuracy scores by cross-validation, a technique in which I evaluate the model’s performance on different subsets of the data. By averaging these scores for each “k,” I gain insight into which “k” number produces the most effective model for my data. These average scores are saved in a list called knn_scores, which ultimately aids in determining the best “k” value to improve the KNN model’s performance.
•	The next step is plotting the graph for KNN algorithm. I have used the line graph to plot it. The trend is represented by the red line, with each point displaying a “k” number and its related score.
 
Figure 66 Plotting the KNN algorithm(1)

 
Figure 67 Plotting the KNN algorithm (2)

o	I am creating a K Nearest Neighbors classifier with one neighbor, which means it will only use the closest data point. Then I use cross-validation on 15 “splits” of my data, training on some and testing on others. I average the accuracy scores of various tests and add them to a list. Finally, I print out each split’s unique accuracy scores to assess how well the model fared in each test instance.




 
Figure 68 Printing the scores for KNN

o	After that I wanted to print the mean score for KNN.
 
Figure 69 The mean score

o	The mean score = 0.926.


4.2.5	Random forest
o	I am creating a random forest model with 15 decision trees using the RandomForestClassifier from the sklearn library. The model’s performance is then evaluated using cross-validation, which divides the data into ten sections and tests how well the model predicts in each. The accuracy ratings of these tests are stored in the sc variable, indicating how well the random forest model works across different data subsets.

 
Figure 70 Random Forest Generator

o	Printing the Mean Score for random forest

 
Figure 71 Mean Score For RF

o	Using the same GridSearchCV Method to find out the best parameters in Random Forest. The parameters are as follows:
o	N_estimators() : determines the number of decision trees in an ensemble approach such as Random Forest.
o	The other parameters are same as Decision tree.

 
Figure 72 Using GridSearchCv for RF(1)


 
Figure 73 Using GridSearchCv for RF(2)
•	After using the same parameters, we obtain a higher accuracy.i.e 0.947
 
Figure 74 RF with Parameters

•	In terms of accuracy, the Random Forest Classifier outperforms the Decision Tree Classifier. This is due to the Random Forest Classifier using the strengths of a set of Decision Trees to create more accurate predictions. This corresponds to our expectations and helps us enhance our ability to predict things. This collaborative method adds to the observed rise in accuracy, demonstrating the model’s capacity to generalise from training data more effectively. The Random Forest Classifier provides a better level of robustness and accuracy in handling classification tasks by utilising the diversity of numerous trees, demonstrating its appropriateness for complicated data scenarios.

•	As I was working on my random forest classifier, I had an idea of measuring its accuracy by converting the cluster count to the number of columns, which would yield the accuracy of each column in the random forest classifier.


 
Figure 75 Process of the code


 
Figure 76 Plotting Random Forest Using No. Of Columns

•	In this graph we can see that as the number of columns increases, we obtain higher accuracy.



4.3	Comparison Between the Data Mining Techniques 


•	The following table gives us the proper comparison between all the techniques used.
No.	Data Mining Technique	Accuracy
1	Logistic Regression	74.1%
2	Decision Tree	78.8%
3	Decision Tree (Tuned)	79.2%
4	Random Forest	93.5%
5	Random Forest (Tuned)	94.1%
6	K Nearest Neighbor	92.6%
Table 5 Comparison


	A detailed evaluation of a dataset using various data mining approaches in the research of predicting heart disease revealed a range of accuracy levels. Beginning with Logistic Regression, the initial accuracy gained was 74.1%, allowing for a basic comprehension of the data's trends. Moving further, Decision Tree analysis produced a 78.8% accuracy, demonstrating its capacity to categorize data hierarchically. Through painstaking fine-tuning of the Decision Tree technique, this accuracy was raised to 79.2%.
	The use of Random Forest, a technique that mixes many decision trees and generated an accuracy of 93.5%, resulted in a significant leap in accuracy. This remarkable performance was improved to 94.1% after optimization. This emphasizes the importance of parameter adjustment in improving prediction performance. Furthermore, the K Nearest Neighbor algorithm achieved a high level of accuracy of 92.6%, demonstrating its ability to detect patterns based on closeness in the heart disease dataset.
	These accuracy disparities highlight the need of selecting relevant methodologies and optimizing their parameters to produce accurate heart disease forecasts. These findings not only guide future model selection and development, but they also support data-driven decision-making processes for better insights into heart disease prediction and effective intervention options.
	The following code below gives a detailed visualization about all the data mining techniques with their accuracies.




 
Figure 77 Classifier
 

 
Chapter 5	
Conclusion and Future Work
5.1	Conclusion 
In conclusion, this dissertation has successfully addressed the core topic of translating data into actionable insights to inform intelligent clinical decisions in the context of heart disease prediction. The rigorously followed goals have resulted in the production of a comprehensive dataset that draws from a variety of sources and is then subjected to rigorous preprocessing to ensure analytical robustness. The subsequent development of a prediction model, utilizing a broad range of data mining approaches, has successfully identified detailed patterns and connections among an extensive set of risk indicators.
The noticeable variation in accuracy achieved by different data mining approaches is a key revelation gleaned from the comparison analysis, as illustrated in the Comparison Table above. The graphic illustration clearly highlights the significant disparity in accuracy across different methodologies. Decision Tree (Tuned), Logistic Regression, and K Nearest Neighbor (KNN) stand out as strong contenders. Decision Tree (Tuned) has a commendable accuracy of 79.2%, demonstrating its capacity for capturing intricate relationships within data. With an accuracy of 74.1%, Logistic Regression demonstrates its usefulness as a basic tool for initial insights. K Nearest Neighbor (KNN) exhibits the usefulness of proximity-based analysis in the field of heart disease prediction, with an outstanding accuracy of 92.6%.
Furthermore, the successful distribution of research findings through respected scientific outlets contributes to the study's influence. This research aims to support more informed decision-making within the medical community by contributing to the collective body of information regarding cardiac disease prediction. The entire strategy taken, as indicated by the collaborative efforts of data collecting, preprocessing, modelling, and evaluation, reveals the path towards harnessing data-driven insights to make proactive and successful clinical decisions.

5.2	Future Work 
The proposed Heart Disease Predictive Method has a lot of potential as an instructional tool for nurses and medical students, making it easier to identify heart disease patients. Furthermore, it has the potential to provide essential decision assistance to doctors by supplementing treatment decisions with increased insights or offering a supplementary "second opinion." The upcoming mathematical study will zero in on the critical characteristics required for identifying heart disease in a diverse population. The predictive paradigm for heart disease could be expanded to include related conditions to broaden the project's scope. The prediction powers might be diversified and amplified by collecting patient data from medical institutions and establishing a web-based testing portal or data extraction tool like the heart disease project. In response to increasing demand, the system's capacity may be increased by adding extra nodes to reduce execution time and accommodate larger datasets.
One promising method is to improve algorithmic efficiency by integration or combination of algorithms into a cohesive system. While this method may not be appropriate for poorly specified datasets, it can speed up decision-making while maintaining precision. The goal remains twofold: to identify critical trends and qualities within patient medical records using a combination of data mining techniques and substantial data analytics, and to predict heart failure before medical intervention is required. Furthermore, simplifying the dataset and improving predictive model performance emerge as concurrent goals.
In the pursuit of effective heart disease prediction, maintaining the highest level of accuracy remains paramount. As a result, determining the single most influential attribute or a synergistic mix of traits that might achieve a 100% predicted accuracy rate for heart disease is a vital area for future research. This ambitious project has the potential to reshape the predictive modelling landscape by providing a critical basis for optimizing and fine-tuning predictive models. By focusing on this distinguishing feature, predicted accuracy could be raised to unprecedented heights, revolutionizing diagnostic precision and, eventually, improving patient care and outcomes. While difficult, this approach could provide profound findings, establishing a new paradigm for predictive analytics in the field of cardiac health.

 
References	
The references will follow an IEEE style (or any other standard referencing format such as Harvard, APA, among others). 
REF:
[1]SASETTY, S. (2020) SUHAS004/heart-disease-predictor-using-artificial-neural-networks, GitHub. Available at: https://github.com/suhas004/Heart-Disease-Predictor-using-Artificial-Neural-Networks (Accessed: 28 June 2023). 
[4]Rajkomar, A. et al. (2018) Scalable and accurate deep learning with electronic health records, Nature News. Available at: https://www.nature.com/articles/s41746-018-0029-1#citeas (Accessed: 10 July 2023). 
[8] Sivagowry, S., Durairaj, M. and Persia, A. (2013) ‘An empirical study on applying data mining techniques for the analysis and prediction of heart disease’, 2013 International Conference on Information Communication and Embedded Systems (ICICES) [Preprint]. doi:10.1109/icices.2013.6508204. 
[10] M, S. (2021) Heart disease prediction using KNN -the K-nearest neighbours algorithm, Analytics Vidhya. Available at: https://www.analyticsvidhya.com/blog/2021/07/heart-disease-prediction-using-knn-the-k-nearest-neighbours-algorithm/ (Accessed: 16 August 2023). 
[11] Sandip Jain (2023) Data preprocessing in Data Mining, GeeksforGeeks. Available at: https://www.geeksforgeeks.org/data-preprocessing-in-data-mining/ (Accessed: 16 August 2023). 
[13] Clopinet, I.G., Guyon, I., Elisseeff, A., et al. (2003) An introduction to variable and feature selection, The Journal of Machine Learning Research. Available at: https://dl.acm.org/doi/10.5555/944919.944968 (Accessed: 18 July 2023). 
[15] Li, S. (2019) Building a logistic regression in Python, step by step, Medium. Available at: https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8 (Accessed: 26 July 2023).
[16] Navlani, A. (2023a) Python decision tree classification tutorial: Scikit-Learn Decisiontreeclassifier, DataCamp. Available at: https://www.datacamp.com/tutorial/decision-tree-classification-python (Accessed: 26 July 2023).  
[17] Navlani, A. (2023b) Python decision tree classification tutorial: Scikit-Learn Decisiontreeclassifier, DataCamp. Available at: https://www.datacamp.com/tutorial/decision-tree-classification-python (Accessed: 26 July 2023). 
[18] Yiu, T. (2021) Understanding random forest, Medium. Available at: https://towardsdatascience.com/understanding-random-forest-58381e0602d2 (Accessed: 26 July 2023). 
[19] Srivastava, T. (2023) A complete guide to K-Nearest Neighbors (updated 2023), Analytics Vidhya. Available at: https://www.analyticsvidhya.com/blog/2018/03/introduction-k-neighbours-algorithm-clustering/ (Accessed: 26 July 2023). 
[20] Japkowicz, N. and Shah, M. (2011). Evaluating Learning Algorithms: A Classification Perspective. Cambridge University Press.
[18] Deshmukh, H. (2020) Heart disease UCI Diagnosis & Prediction, Medium. Available at: https://towardsdatascience.com/heart-disease-uci-diagnosis-prediction-b1943ee835a7#1257 (Accessed: 25 July 2023). 
[23] Shin, T. (2021) An extensive step by step guide to Exploratory Data Analysis, Medium. Available at: https://towardsdatascience.com/an-extensive-guide-to-exploratory-data-analysis-ddd99a03199e (Accessed: 26 July 2023). 
Book
[14]Hastie, T., Friedman, J. and Tisbshirani, R. (2017) The elements of Statistical Learning: Data Mining, Inference, and prediction. New York: Springer
Journal
[2] N., P. and Kumar, P.R. (2017) ‘Usage of data mining techniques in predicting the heart diseases — naïve Bayes & Decision tree’, 2017 International Conference on Circuit ,Power and Computing Technologies (ICCPCT) [Preprint]. doi:10.1109/iccpct.2017.8074215. 
[3] Shouman, M., Turner, T. and Stocker, R. (2012) Using data mining techniques in heart disease diagnosis and treatment. Available at: https://ieeexplore.ieee.org/document/6186978/ (Accessed: 10 July 2023). 
[5] Tsai, D.-Y. and Watanabe, S. (1999) ‘A method for optimization of fuzzy reasoning by genetic algorithms and its application to discrimination of myocardial heart disease’, IEEE Transactions on Nuclear Science, 46(6), pp. 2239–2246. doi:10.1109/23.819310. 
[6] Medithe, J. and Nelakuditi, U.R. (2016) ‘Removal of ocular artifacts in EEG’, 2016 10th International Conference on Intelligent Systems and Control (ISCO) [Preprint]. doi:10.1109/isco.2016.7726941. 
[7] Jabbar, M.A. and Samreen, S. (2016) ‘Heart disease prediction system based on hidden naïve Bayes classifier’, 2016 International Conference on Circuits, Controls, Communications and Computing (I4C) [Preprint]. doi:10.1109/cimca.2016.8053261. 
[9] Ouyang, S. (2022) ‘Research of heart disease prediction based on machine learning’, 2022 5th International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE) [Preprint]. doi:10.1109/aemcse55572.2022.00071. 
[12] P. J. Garcia-Laencina, A. R. Figueiras Vidal and J. -L. Sancho-Gomez, "A robust approach for classifying unknown data in medical diagnosis problems," 2008 World Automation Congress, Waikoloa, HI, USA, 2008, pp. 1-6.
[21] Powers, D.M. (2011). Evaluation: From Precision, Recall and F-Measure to ROC, Informedness, Markedness & Correlation. Journal of Machine Learning Technologies, 2(1), pp.37-63.
[22] Saito, T. and Rehmsmeier, M. (2015). The Precision-Recall Plot Is More Informative than the ROC Plot When Evaluating Binary Classifiers on Imbalanced Datasets. PLoS ONE, 10(3), e0118432.


 
Appendix A.		Title of Appendix
Appendix Heading 1
Text of the appendix goes here

Appendix Heading 2
Text of the appendix goes here 

Appendix Table and Figure Captions
In appendices, table and figure caption labels and numbers are typed in manually (e.g., Table A1, Table A2, etc.). These do not get generated into the lists that appear after the Table of Contents. 

 
Appendix B.		Title of Appendix 
Text of the appendix goes here if there is only a single heading. 



